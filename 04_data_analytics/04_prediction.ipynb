{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Downloading Time Series Data with Polygon-API-Client\n",
    "\n",
    "To start training a model with stock prices and volume to predict the next minute price, we first need to collect and prepare our dataset. In this case, we're interested in downloading 3 months of intraday data with 1-minute intervals using the Polygon API. Let's dive into how to achieve this step by step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Prerequisites\n",
    "\n",
    "Before you begin, ensure you have created a Polygon.io account and installed the necessary Python packages:\n",
    "\n",
    "- `numpy`: For efficient manipulation of large arrays.\n",
    "- `pandas`: For handling and manipulating the dataset.\n",
    "- `polygon-api-client`: To fetch stock market data from Polygon.io.\n",
    "\n",
    "1) Create a **FREE** Polygon.io account at the their sign up page: [https://polygon.io/dashboard/signup](https://polygon.io/dashboard/signup)\n",
    "\n",
    "2) Get your Polygon.io API key at [https://polygon.io/dashboard/api-keys](https://polygon.io/dashboard/api-keys)\n",
    "\n",
    "3) You can install these packages using pip:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy pandas polygon-api-client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Fetching the Data\n",
    "\n",
    "To fetch the data, you'll need an API key from Polygon.io. Once you have your key, you can use the following code snippet to download the stock price data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datetime import datetime, timedelta\n",
    "from polygon import RESTClient\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def fetch_data(symbol:str, start_date:datetime, end_date:datetime, polygon_client: RESTClient):\n",
    "    aggregate_iter = polygon_client.list_aggs(\n",
    "        symbol, 1, \"minute\", start_date, end_date, limit=50_000)\n",
    "    \n",
    "    # Initialize an empty list to store the data\n",
    "    data = []\n",
    "    \n",
    "    # Loop through the returned data, extracting the necessary information\n",
    "    for aggregate in aggregate_iter:\n",
    "        data.append([\n",
    "            # Convert timestamp from milliseconds\n",
    "            pd.to_datetime(aggregate.timestamp, unit='ms'),\n",
    "            aggregate.open,  # Open price\n",
    "            aggregate.high,  # High price\n",
    "            aggregate.low,  # Low price\n",
    "            aggregate.close,  # Close price\n",
    "            aggregate.volume,  # Volume\n",
    "            aggregate.transactions,  # Number of transactions in the aggregate window\n",
    "            aggregate.vwap,  # Volume weighted average price\n",
    "        ])\n",
    "    \n",
    "    # Create a DataFrame\n",
    "    columns = ['timestamp', 'open', 'high', 'low',\n",
    "               'close', 'volume', \"transactions\", \"vwap\"]\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "    return df\n",
    "\n",
    "\n",
    "# Example usage\n",
    "api_key = \"YOUR_API_KEY_HERE\"\n",
    "symbol = \"AAPL\"\n",
    "end_date = datetime.now()\n",
    "start_date = end_date - timedelta(days=120)\n",
    "\n",
    "\n",
    "client = RESTClient(api_key)\n",
    "df = fetch_data(symbol, start_date, end_date, client)\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an updated explanation of the code you provided, which fetches stock price data using the Polygon API:\n",
    "\n",
    "- **Datetime and timedelta usage**: This code imports `datetime` and `timedelta` from the datetime module to calculate the start and end dates for the data fetching. This allows for dynamic date calculations, such as fetching the last 90 days of data up to the current date.\n",
    "  \n",
    "- **Polygon RESTClient**: The `RESTClient` from the polygon package is initialized outside of the `fetch_data` function and passed as an argument. This allows for more flexible usage of the client, such as reusing the same client for multiple data fetch operations without reinitializing it each time.\n",
    "  \n",
    "- **Fetching data with list_aggs method**: Instead of `stocks_equities_aggregates`, this code uses the `list_aggs` method to fetch aggregated stock data. This method is called with the stock symbol, aggregation size (1 minute), and the start and end dates. The `limit=50_000` parameter specifies the maximum number of results to return, accommodating large datasets.\n",
    "  \n",
    "- **Data extraction and conversion**: As before, the code loops through each aggregate in the fetched data. However, it now also extracts the `transactions` (the number of transactions within each minute) and `vwap` (volume-weighted average price), providing a more detailed dataset.\n",
    "  \n",
    "- **DataFrame creation**: The extracted data is stored in a list and then used to create a pandas DataFrame. This DataFrame includes columns for timestamp, open, high, low, close, volume, transactions, and vwap. This structure facilitates easy manipulation and analysis of the stock data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try it Out\n",
    "\n",
    "Experiment with fetching data for different symbols or adjusting the date range to explore how the market has performed over different periods. For instance, you can change the `symbol` to \"AAPL\" for Apple Inc. or adjust the `timedelta` to fetch data over a different duration. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust the symbol or date range\n",
    "start_date = end_date - timedelta(days=180)  # Example: Change to fetch the last 180 days\n",
    "df = fetch_data(symbol, start_date, end_date, client)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In the next steps, we will preprocess this data to be suitable for training our model with TensorFlow and Scikit-Learn, focusing on using stock prices and volume to predict the next minute price."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Plotting Candlestick Charts with Matplotlib\n",
    "\n",
    "Candlestick charts are a popular method for visualizing stock price movements over time. Each candlestick provides information on the open, high, low, and close prices for a given time period. In this section, we'll show how to plot candlestick charts using Matplotlib and the data we've fetched in the previous step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Prerequisites\n",
    "\n",
    "First, ensure you have the necessary package for plotting:\n",
    "\n",
    "- `matplotlib`: For creating visualizations in Python.\n",
    "- `mplfinance`: A Matplotlib utility specifically for financial data visualization, including candlestick charts.\n",
    "\n",
    "You can install these packages using pip:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib mplfinance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the Data\n",
    "\n",
    "Before plotting, ensure your DataFrame is correctly formatted for `mplfinance`. The DataFrame's index must be a DatetimeIndex, so we'll set the 'timestamp' column as the index:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index('timestamp', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Plotting the Candlestick Chart\n",
    "\n",
    "Now, let's plot the candlestick chart for our stock data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mplfinance as mpf\n",
    "\n",
    "last_timestamp = df.index.max()  # Get the most recent timestamp\n",
    "# Calculate one hour before the last timestamp\n",
    "one_hour_ago = last_timestamp - pd.Timedelta(hours=1)\n",
    "\n",
    "# Filter the DataFrame for the last hour\n",
    "df_last_hour = df[one_hour_ago:]\n",
    "\n",
    "# Plotting\n",
    "mpf.plot(df_last_hour, type='candle', style='charles',\n",
    "         title='Stock Price Candlestick Chart - Last Hour',\n",
    "         ylabel='Price ($)',\n",
    "         volume=True,\n",
    "         ylabel_lower='Volume',\n",
    "         figratio=(12, 6),\n",
    "         mav=(3,6,9))  # Moving averages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Understanding the Plotting Code\n",
    "\n",
    "- **mpf.plot() Function**: This function from `mplfinance` is used to plot the candlestick chart. We pass our DataFrame `df` as the first argument.\n",
    "  \n",
    "- **type='candle'**: Specifies that we want to plot a candlestick chart.\n",
    "  \n",
    "- **style='charles'**: Chooses a predefined style for the chart. `mplfinance` offers several styles; 'charles' is just one example.\n",
    "  \n",
    "- **title, ylabel, and ylabel_lower**: These parameters set the title of the chart and labels for the y-axes. Since we're plotting volume as well, `ylabel_lower` is used for the volume subplot.\n",
    "  \n",
    "- **volume=True**: This includes a volume chart beneath the candlestick chart.\n",
    "  \n",
    "- **figratio**: Sets the width and height ratio of the figure. This can be adjusted based on your preference or to fit the data better.\n",
    "  \n",
    "- **mav**: Specifies the periods for which to calculate and plot moving averages. This example plots moving averages for 3, 6, and 9 periods. You can adjust these values or omit this parameter if you don't need moving averages.\n",
    "\n",
    "### Try it Out\n",
    "\n",
    "Experiment with different styles, moving average periods, or even zooming in on specific dates to analyze the stock's performance more closely. `mplfinance` offers various customization options to explore:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zooming in on a specific date range\n",
    "mpf.plot(df['2024-02-01':'2024-03-31'], type='candle', style='charles', ... )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Replace the `...` with the rest of the parameters from the previous example or try new ones to customize the plot further. This visual analysis can provide valuable insights into market trends and help inform your model development in the next stages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Building the AI for Stock Price Prediction (Multi-variable LSTM Model)\n",
    "\n",
    "Long Short-Term Memory (LSTM) networks are a type of recurrent neural network (RNN) capable of learning order dependence in sequence prediction problems. This makes them ideal for time series forecasting like stock price predictions. In this tutorial, we'll create a multi-variable LSTM model to predict stock prices in the next minute using TensorFlow and Keras.\n",
    "\n",
    "Before we be let's make sure we install a few more machine learning packages:\n",
    "\n",
    "- `tensorflow`: Enables building and training complex neural network models for deep learning applications.\n",
    "- `scikit-learn`: Provides a wide range of simple and efficient tools for data mining and data analysis, including preprocessing and model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install tensorflow scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Overview\n",
    "\n",
    "Our dataset includes variables such as open, high, low, close prices, volume, transactions, and volume-weighted average price (VWAP). We'll use these features to predict the closing price in the next minute.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Preparing the Data\n",
    "\n",
    "First, we need to preprocess our data to fit the LSTM model. This involves scaling the data, creating a time series dataset with the appropriate sequence length, and splitting the dataset into training and test sets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Scaling the Data\n",
    "\n",
    "LSTMs are sensitive to the scale of the input data. It's common practice to normalize or standardize the data before training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "# Select features to scale\n",
    "features = ['open', 'high', 'low', 'close', 'volume', \"transactions\", \"vwap\"]\n",
    "data = df[features]\n",
    "\n",
    "# Use MinMaxScaler to scale the data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "# Convert scaled data back to a DataFrame for easier manipulation\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary**:\n",
    "This code snippet is an essential step in data preprocessing for machine learning models, especially when dealing with time series data like stock prices. Scaling the data to a common scale allows the model to train more effectively, as it ensures that no single feature will dominate the learning process due to its scale.\n",
    "\n",
    "<details>\n",
    "\n",
    "<summary>ðŸ’¡ Click me for more details on the code</summary>\n",
    "\n",
    "This code is designed to scale the features of a stock price dataset, preparing it for input into machine learning models, such as the LSTM model for predicting future stock prices. Let's break down each part of the code for a clearer understanding:\n",
    "\n",
    "1. **Importing Required Libraries**:\n",
    "   - `from sklearn.preprocessing import MinMaxScaler`: Imports the `MinMaxScaler` class from scikit-learn, a popular machine learning library. `MinMaxScaler` is used for scaling features to a given range.\n",
    "   - `import numpy as np`: Imports the NumPy library, which is fundamental for scientific computing in Python. NumPy provides support for large, multi-dimensional arrays and matrices, along with a collection of mathematical functions to operate on these arrays.\n",
    "\n",
    "2. **Selecting Features to Scale**:\n",
    "   - `features = ['open', 'high', 'low', 'close', 'volume', \"transactions\", \"vwap\"]`: Defines a list of column names from the DataFrame `df` that will be scaled. These columns represent different attributes of stock prices, including the opening price (`open`), the highest price in the interval (`high`), the lowest price in the interval (`low`), the closing price (`close`), the number of shares traded (`volume`), the number of transactions (`transactions`), and the volume-weighted average price (`vwap`).\n",
    "\n",
    "3. **Extracting Data for Scaling**:\n",
    "   - `data = df[features]`: Extracts the columns specified in the `features` list from the DataFrame `df`. This creates a new DataFrame `data` containing only the columns that need to be scaled.\n",
    "\n",
    "4. **Scaling the Data**:\n",
    "   - `scaler = MinMaxScaler(feature_range=(0, 1))`: Creates an instance of the `MinMaxScaler` class. The parameter `feature_range=(0, 1)` specifies that the scaled values should fall within the range of 0 to 1.\n",
    "   - `scaled_data = scaler.fit_transform(data)`: The `fit_transform` method computes the minimum and maximum values of `data` to perform the scaling (fit part) and then scales the `data` (transform part). The result is a NumPy array `scaled_data` where each original value in `data` is scaled to a value between 0 and 1.\n",
    "\n",
    "5. **Converting Scaled Data Back to DataFrame**:\n",
    "   - `scaled_df = pd.DataFrame(scaled_data, columns=features)`: Converts the scaled data, which is a NumPy array, back into a pandas DataFrame for easier manipulation in future steps. The `columns=features` argument ensures that the columns in `scaled_df` have the same names as the original DataFrame `df`, maintaining consistency and making it easier to understand which column represents what feature.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Creating Sequences\n",
    "\n",
    "Next, we'll create sequences of data points to use as inputs for the LSTM. Each input sequence will be used to predict the closing price in the next minute.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, sequence_length):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        X.append(data[i:(i + sequence_length)])\n",
    "        y.append(data[i + sequence_length, 3])  # Assuming 'close' is at index 3\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "sequence_length = 60  # Number of minutes to use for prediction\n",
    "X, y = create_sequences(scaled_df.values, sequence_length)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Summary**:\n",
    "This code prepares the input and output data for training an LSTM network by organizing the scaled time series data into sequences. Each sequence represents a fixed period (in this case, 60 minutes) used by the network to predict the closing price in the next time step. This approach is crucial for capturing temporal dependencies in time series data, enabling more accurate predictions in tasks like stock price forecasting.\n",
    "\n",
    "<details>\n",
    "\n",
    "<summary>ðŸ’¡ Click me for more details on the code</summary>\n",
    "\n",
    "This code snippet is focused on transforming the scaled stock price data into sequences that can be used for training a Long Short-Term Memory (LSTM) network. LSTM networks require input data in the form of sequences for making predictions based on time series data. Here's a detailed explanation of each part of the code:\n",
    "\n",
    "1. **Defining the `create_sequences` Function**:\n",
    "   - The function `create_sequences(data, sequence_length)` takes two parameters: `data`, which is a dataset containing scaled features, and `sequence_length`, which defines the length of the sequences to be created. The `sequence_length` is essentially the number of time steps the LSTM will look back to make a prediction.\n",
    "\n",
    "2. **Initializing Empty Lists for Sequences and Labels**:\n",
    "   - `X, y = [], []`: Initializes two empty lists, `X` and `y`. `X` will store the input sequences, and `y` will store the corresponding labels (or targets) for each sequence. In the context of stock price prediction, a sequence in `X` consists of stock prices and other features over a specified period, while the corresponding label in `y` is the price we want to predict at the next time step.\n",
    "\n",
    "3. **Creating Sequences and Corresponding Labels**:\n",
    "   - The `for` loop iterates through the `data`, creating sequences of length `sequence_length` and appending them to `X`. For each sequence in `X`, it also appends the next value of the closing price (assumed to be at index 3 in the `data` array) to `y` as the label.\n",
    "   - `data[i:(i + sequence_length)]`: This slice of `data` takes `sequence_length` consecutive values starting from index `i`, creating a sequence that is added to `X`.\n",
    "   - `data[i + sequence_length, 3]`: This accesses the closing price (`close`), which is immediately after the end of the current sequence, and adds it to `y` as the label for the sequence. The assumption here is that the 'close' price is at index 3 of the `data` array.\n",
    "\n",
    "4. **Converting Lists to NumPy Arrays**:\n",
    "   - `return np.array(X), np.array(y)`: The function returns two NumPy arrays, `X` and `y`. Converting the lists to NumPy arrays is important for compatibility with TensorFlow and Keras, which are used to build and train the LSTM model. NumPy arrays provide efficient storage and computation capabilities, especially for large datasets.\n",
    "\n",
    "5. **Specifying the Sequence Length**:\n",
    "   - `sequence_length = 60`: This sets the `sequence_length` to 60, meaning each sequence will contain data for 60 minutes. This parameter can be adjusted based on the frequency of the data and the specific requirements of the prediction task.\n",
    "\n",
    "6. **Generating Sequences and Labels**:\n",
    "   - `X, y = create_sequences(scaled_df.values, sequence_length)`: Calls the `create_sequences` function with the scaled data (`scaled_df.values`) and the specified `sequence_length` to generate the input sequences (`X`) and their corresponding labels (`y`).\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the Dataset\n",
    "\n",
    "Split the dataset into training and test sets. Typically, we use the most recent data for testing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split = int(0.8 * len(X))\n",
    "X_train, X_test = X[:train_split], X[train_split:]\n",
    "y_train, y_test = y[:train_split], y[train_split:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary**:\n",
    "The purpose of this code is to partition the dataset into training and test sets, a common practice in machine learning to evaluate model performance. By training the model on a subset of the data (`X_train` and `y_train`) and testing it on unseen data (`X_test` and `y_test`), we can assess how well the model generalizes to new, unseen data points. This step is essential for avoiding overfitting, where a model might perform well on training data but poorly on new data, and for ensuring that the model can make accurate predictions in real-world situations.\n",
    "\n",
    "<details>\n",
    "\n",
    "<summary>ðŸ’¡ Click me for more details on the code</summary>\n",
    "\n",
    "This code snippet is about splitting the sequences of stock price data into training and test sets. This is a crucial step in preparing the data for training and evaluating a machine learning model, such as an LSTM network for time series prediction. Here's a detailed breakdown of each part of the code:\n",
    "\n",
    "1. **Determining the Training Set Size**:\n",
    "   - `train_split = int(0.8 * len(X))`: This line calculates the size of the training set. It multiplies the total number of sequences (`len(X)`) by 0.8 to use 80% of the data for training. The `int()` function is used to ensure that the result is an integer, as the number of sequences must be a whole number. This split ratio can be adjusted based on the specific requirements of the model or to ensure that the model has enough data for effective training and validation.\n",
    "\n",
    "2. **Splitting the Sequences into Training and Test Sets**:\n",
    "   - `X_train, X_test = X[:train_split], X[train_split:]`: This line splits the input sequences (`X`) into a training set (`X_train`) and a test set (`X_test`). The training set contains the first 80% of the sequences, as determined by `train_split`, while the test set contains the remaining 20%. This split allows the model to learn from the training data and then be evaluated on unseen data from the test set to gauge its predictive performance.\n",
    "   \n",
    "   - `y_train, y_test = y[:train_split], y[train_split:]`: Similarly, this line splits the labels (`y`) into a training set (`y_train`) and a test set (`y_test`) using the same `train_split` index. This ensures that each input sequence in `X_train` and `X_test` has a corresponding label in `y_train` and `y_test`, respectively.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Building the LSTM Model\n",
    "\n",
    "Now, let's construct our LSTM model using TensorFlow and Keras.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "model = Sequential([\n",
    "    LSTM(50, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "    Dropout(0.2),\n",
    "    LSTM(50, return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "    LSTM(50),\n",
    "    Dropout(0.2),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary**:\n",
    "This model is structured to capture the temporal dependencies in the time series data of stock prices through its LSTM layers, while also employing regularization via Dropout layers to mitigate the risk of overfitting. The final Dense layer outputs the predicted stock price, and the model is compiled with a configuration suited for regression tasks. Training this model involves fitting it to a sequence of stock prices and volumes, aiming to predict the stock price at the next minute accurately.\n",
    "\n",
    "<details>\n",
    "\n",
    "<summary>ðŸ’¡ Click me for more details on the code</summary>\n",
    "\n",
    "This code snippet is focused on building a Sequential model using TensorFlow's Keras API, specifically designed for a time series prediction task like forecasting stock prices. The model employs Long Short-Term Memory (LSTM) layers, which are well-suited for learning from sequences of data, along with Dropout layers to prevent overfitting. Here's a detailed explanation of each part:\n",
    "\n",
    "1. **Importing Required Modules**:\n",
    "   - `from tensorflow.keras.models import Sequential`: Imports the `Sequential` model class from TensorFlow's Keras API. A `Sequential` model is a linear stack of layers.\n",
    "   - `from tensorflow.keras.layers import LSTM, Dense, Dropout`: Imports the `LSTM`, `Dense`, and `Dropout` layer classes. `LSTM` layers are used for sequence prediction problems, `Dense` layers are fully connected neural network layers, and `Dropout` layers help prevent overfitting by randomly setting input units to 0 during training.\n",
    "   - `from tensorflow.keras.optimizers import Adam`: Imports the `Adam` optimizer, a popular algorithm for training neural networks.\n",
    "\n",
    "2. **Building the Sequential Model**:\n",
    "   - `model = Sequential([...])`: Initializes a new Sequential model. The model is defined by passing a list of layers to the `Sequential` constructor. This list specifies the architecture of the neural network.\n",
    "\n",
    "3. **Defining the Model Architecture**:\n",
    "   - The model architecture consists of three LSTM layers and three Dropout layers, ending with a Dense layer.\n",
    "   - `LSTM(50, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2]))`: The first layer is an LSTM layer with 50 units. The `return_sequences=True` argument is necessary for stacking LSTM layers so that the subsequent LSTM layer receives sequences as input. The `input_shape` is specified according to the shape of the training data, allowing the model to know the input dimensionality.\n",
    "   - `Dropout(0.2)`: Following each LSTM layer (except the last), a Dropout layer is added with a rate of 0.2, meaning 20% of the input units are randomly set to 0 during training, which helps in preventing overfitting.\n",
    "   - `Dense(1)`: The final layer is a Dense layer with a single unit. In the context of stock price prediction, this output corresponds to the predicted value of the stock price at the next time step.\n",
    "\n",
    "4. **Compiling the Model**:\n",
    "   - `model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')`: The model is compiled with the Adam optimizer, with a learning rate of 0.001, and the mean squared error loss function. The choice of loss function is appropriate for regression problems, where the goal is to minimize the difference between the predicted and actual values.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "\n",
    "Train the model using the training data. Depending on your dataset size, this might take some time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def create_unique_file(base_filename, extension=''):\n",
    "    counter = 1\n",
    "    filename = f\"{base_filename}{extension}\"\n",
    "    while os.path.exists(filename):\n",
    "        filename = f\"{base_filename}_{counter}{extension}\"\n",
    "        counter += 1\n",
    "\n",
    "    return filename\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=7, batch_size=32, validation_split=0.1, verbose=1)\n",
    "\n",
    "# Create a unique file name for the model\n",
    "filename = create_unique_file(f\"{symbol}_lstm_model\", \".keras\")\n",
    "\n",
    "# Save your model for later\n",
    "model.save(filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary**: This code snippet efficiently trains a neural network model and then saves it in a way that ensures the filename is unique. This is particularly useful when training multiple models in a script or when working in directories with many files, as it avoids accidental data loss due to file overwriting.\n",
    "\n",
    "<details>\n",
    "\n",
    "<summary>ðŸ’¡ Click me for more details on the code</summary>\n",
    "\n",
    "This code snippet demonstrates how to train a machine learning model using TensorFlow and Keras, and then save the trained model to a unique file for later use. It also includes a custom function to ensure that the saved file does not overwrite any existing files. Let's break down the code for better understanding:\n",
    "\n",
    "**Part 1: Training the Model**\n",
    "\n",
    "- `history = model.fit(X_train, y_train, epochs=7, batch_size=32, validation_split=0.1, verbose=1)`: This line trains the model on the training data (`X_train`, `y_train`) over 7 epochs, with a batch size of 32. During training, 10% of the training data is held back as a validation set (`validation_split=0.1`) to monitor the model's performance on unseen data. The `verbose=1` argument specifies that the training process will output detailed information about its progress after each epoch.\n",
    "\n",
    "**Part 2: Defining a Function to Create a Unique Filename**\n",
    "\n",
    "- `import os`: Imports the `os` module, which provides functions for interacting with the operating system, including checking whether a file exists.\n",
    "\n",
    "- `def create_unique_file(base_filename, extension='')`: Defines a function that takes a base filename and an optional extension as arguments. The purpose of this function is to generate a unique filename by appending a counter to the base filename if a file with the proposed name already exists.\n",
    "\n",
    "  - `counter = 1`: Initializes a counter used to generate unique filenames if needed.\n",
    "  - `filename = f\"{base_filename}{extension}\"`: Constructs the initial filename using the provided base filename and extension.\n",
    "  - `while os.path.exists(filename)`: Checks if a file with the current filename exists. If it does, the loop constructs a new filename with an appended counter (`filename = f\"{base_filename}_{counter}{extension}\"`) and increments the counter. This loop continues until a unique filename is found.\n",
    "  - `return filename`: Returns the unique filename.\n",
    "\n",
    "**Part 3: Saving the Trained Model**\n",
    "\n",
    "- `filename = create_unique_file(f\"{symbol}_lstm_model\", \".keras\")`: Calls the `create_unique_file` function with the base filename as `\"{symbol}_lstm_model\"` and the extension as `\".keras\"`. The `symbol` variable is assumed to be a string that represents the stock symbol the model was trained on. This step ensures that the model is saved to a unique file, preventing any existing files from being overwritten.\n",
    "\n",
    "- `model.save(filename)`: Saves the trained model to the file with the generated unique filename. This allows the model to be loaded and reused later, without needing to retrain.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Predicting and Evaluating the Model\n",
    "\n",
    "Finally, use the model to make predictions on the test set and evaluate its performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "test_loss = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Calculate performance metrics\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "rmse = sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary**: The code snippet evaluates a machine learning model's performance on unseen test data by calculating the test loss, making predictions, and quantifying prediction errors using key metrics (MSE, RMSE, MAE). These steps are essential for assessing the model's predictive accuracy and for guiding improvements.\n",
    "\n",
    "<details>\n",
    "\n",
    "<summary>ðŸ’¡ Click me for more details on the code</summary>\n",
    "\n",
    "This code snippet is focused on evaluating the performance of a trained neural network model, specifically for a regression task such as stock price prediction. It evaluates the model's loss on the test dataset, makes predictions, and calculates key performance metrics. Here's a breakdown of each part:\n",
    "\n",
    "**Part 1: Importing Required Libraries**\n",
    "\n",
    "- `import matplotlib.pyplot as plt`: Imports the Matplotlib library for plotting, which is not directly used in the given snippet but is commonly used for visualizing data and model performance.\n",
    "- `from sklearn.metrics import mean_squared_error, mean_absolute_error`: Imports functions to calculate the mean squared error (MSE) and mean absolute error (MAE) from scikit-learn, a machine learning library. These metrics are used to quantify the model's prediction errors.\n",
    "- `from math import sqrt`: Imports the `sqrt` function from the math module to calculate the square root, which is used for computing the root mean squared error (RMSE).\n",
    "\n",
    "**Part 2: Evaluating the Model**\n",
    "\n",
    "- `test_loss = model.evaluate(X_test, y_test, verbose=0)`: This line evaluates the trained model on the test dataset (`X_test`, `y_test`) without printing the evaluation output (`verbose=0`). The evaluation returns the loss value (in this case, mean squared error, as specified during model compilation) on the test data. The loss value quantifies how well the model's predictions match the actual labels.\n",
    "\n",
    "- `print(f\"Test Loss: {test_loss}\")`: Prints the loss of the model on the test data, providing a high-level quantification of the model's prediction error.\n",
    "\n",
    "**Part 3: Making Predictions**\n",
    "\n",
    "- `predictions = model.predict(X_test)`: Uses the trained model to make predictions on the test dataset. The output is a numpy array of predicted values corresponding to the input samples in `X_test`.\n",
    "\n",
    "**Part 4: Calculating Performance Metrics**\n",
    "\n",
    "- `mse = mean_squared_error(y_test, predictions)`: Calculates the mean squared error between the actual labels (`y_test`) and the predicted values (`predictions`). MSE is the average of the squares of the errors and is commonly used to measure the accuracy of continuous variables.\n",
    "\n",
    "- `rmse = sqrt(mse)`: Calculates the root mean squared error, which is the square root of MSE. RMSE is a measure of the accuracy that is in the same units as the response variable. It's particularly useful because it gives a relatively high weight to large errors.\n",
    "\n",
    "- `mae = mean_absolute_error(y_test, predictions)`: Calculates the mean absolute error, which is the average of the absolute differences between the predictions and actual values. Unlike MSE or RMSE, MAE provides a linear measure of prediction accuracy.\n",
    "\n",
    "- `print` statements: These lines print out the calculated MSE, RMSE, and MAE, providing a detailed overview of the model's prediction performance. These metrics are crucial for understanding the model's accuracy and for comparing it with other models.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Predictions\n",
    "\n",
    "It's helpful to visualize the predictions against the actual closing prices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot predictions vs actual values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(y_test, label='Actual')\n",
    "plt.plot(predictions, label='Predicted')\n",
    "plt.title('Model Predictions vs Actual Values')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Close Price')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary**: This code snippet effectively visualizes the comparison between the actual closing prices and the predicted prices generated by a machine learning model. Visual comparison helps in quickly assessing the model's predictive performance and understanding how closely the predictions align with the actual values over the given time period.\n",
    "\n",
    "<details>\n",
    "\n",
    "<summary>ðŸ’¡ Click me for more details on the code</summary>\n",
    "\n",
    "This code snippet is about visualizing the comparison between actual values and model predictions, specifically targeting a regression problem like stock price forecasting. By plotting both sets of data on the same graph, it provides an intuitive understanding of the model's performance. Here's a step-by-step explanation:\n",
    "\n",
    "### Part 1: Importing Matplotlib\n",
    "\n",
    "- `import matplotlib.pyplot as plt`: This imports Matplotlib's `pyplot` module, a collection of functions that make Matplotlib work like MATLAB, allowing for interactive plots and simple visualizations. Matplotlib is a widely used Python library for data visualization.\n",
    "\n",
    "### Part 2: Setting Up the Plot\n",
    "\n",
    "- `plt.figure(figsize=(10, 6))`: Creates a new figure for plotting with a specified size of 10 inches in width and 6 inches in height. The `figsize` parameter ensures that the plot has enough space to clearly display the data without squeezing the labels and titles.\n",
    "\n",
    "### Part 3: Plotting Data\n",
    "\n",
    "- `plt.plot(y_test, label='Actual')`: Plots the actual values (`y_test`) on the figure. These values represent the true closing prices from the test dataset. The `label='Actual'` argument assigns a label to this line for identification in the legend.\n",
    "\n",
    "- `plt.plot(predictions, label='Predicted')`: Plots the predicted values obtained from the model on the same figure. The predictions are the model's output based on the input features from the test dataset. Similar to the actual values, a label is assigned for identification in the legend.\n",
    "\n",
    "### Part 4: Customizing the Plot\n",
    "\n",
    "- `plt.title('Model Predictions vs Actual Values')`: Sets the title of the plot to \"Model Predictions vs Actual Values,\" which describes the purpose of the plot â€” comparing the model's predictions against the actual closing prices.\n",
    "\n",
    "- `plt.xlabel('Time')`: Labels the x-axis as \"Time.\" In this context, \"Time\" represents the sequence of data points, although it does not specify the exact time frame due to the generic nature of the plot. For more detailed time series plots, you might consider plotting actual datetime values on the x-axis.\n",
    "\n",
    "- `plt.ylabel('Close Price')`: Labels the y-axis as \"Close Price,\" indicating that the values being plotted represent the closing prices of the stock.\n",
    "\n",
    "- `plt.legend()`: Adds a legend to the plot. The legend uses the labels defined earlier (`'Actual'` and `'Predicted'`) to distinguish between the two lines plotted on the graph.\n",
    "\n",
    "### Part 5: Displaying the Plot\n",
    "\n",
    "- `plt.show()`: Displays the figure. This command renders the plot and shows it in a window. In Jupyter notebooks and other similar environments, this will display the plot inline.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "You've now built and trained a multi-variable LSTM model to predict stock prices. Remember, stock market prediction is inherently uncertain and influenced by many external factors. Always consider this when evaluating model performance.\n",
    "\n",
    "### What's next? ðŸ§ª Experiment!\n",
    "\n",
    "This is just the starting point. Try changing the parameters we used to build and train the model. Here are some suggestions:\n",
    "\n",
    "**In the build phase:**\n",
    "- **Layers**: Experiment with adding or removing layers. A comprehensive list of available layers can be found on the Keras documentation website at [https://keras.io/api/layers/](https://keras.io/api/layers/). Each layer type offers different functionalities, such as `Conv1D` for convolutional operations on sequences or `Bidirectional` to make your LSTM layers process the sequences in both directions.\n",
    "- **Layer Parameters**: Try modifying the parameters of the existing layers. For example, you can adjust the number of units in LSTM layers or the dropout rate in Dropout layers to see how they impact model performance.\n",
    "\n",
    "**In the train phase:**\n",
    "- **Epochs**: Increase this to add more training phases. More epochs mean the model will have more opportunities to learn from the data, but watch out for overfitting.\n",
    "- **Batch Size**: Experiment with the batch size. A smaller batch size might increase training time but can lead to a more stable and fine-tuned learning process.\n",
    "- **Validation Split**: Adjust the percentage of data used for validation during training. Finding the right balance between training and validation data can help in building a model that generalizes well.\n",
    "\n",
    "**Evaluation and Metrics:**\n",
    "- Consider using additional metrics for evaluation, such as `R^2` score, to get a better sense of how well your model is performing.\n",
    "- Plot learning curves to visualize the model's learning process over time and identify if and when the model starts overfitting.\n",
    "\n",
    "By experimenting with different configurations, you'll gain a deeper\n",
    "understanding of how various aspects of the model and the training process\n",
    "affect performance. Happy modeling!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
